\chapter{Classification and evaluation}
This chapter presents the classification model used in our task, the validation techniques and the features selection algorithm applied to our data.
\section{Random Forest}
Random forest is an ensemble learning method for classification, regression, and other tasks that operates by constructing multiple decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. \cite{ho1995random}
\subsection{Decision tree}
A decision tree is a method used in different machine learning tasks. It uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements. \cite{kaminski2018framework}

Each graph's nodes represent a test on a different feature, every branch represents the outcome of the previous test, and each leaf represents the decision after analyzing all the features, i.e., the class label. 

Unfortunately, the deeper is the graph, the higher are the chances of overfitting the training test.
Random forest is a method to average multiple decision trees, trained on different chunks of the training set, to reduce the variance of the output. However, this comes with a cost, an increase of the bias, and a decrease in results interpretability. \cite{hastie2009elements}
\subsection{Bagging}

Bagging, also known as \textbf{B}ootstrap \textbf{Agg}regat\textbf{ing}, is a machine learning meta-algorithm used to improve the accuracy of a model, reducing the model's variance and the likelihood of overfitting. 

The noise in the training set affects the prediction of a single tree, but it does not affect multiple trees, as long as they are not correlated to each other. Training multiple decision trees on the same training set would produce trees highly correlated to each other. Instead, with the bootstrap aggregation technique, we can de-correlate the trees by training them on different parts of the training set. \cite{breiman1996bagging}

Given a training set $X = x_1,\dots,x_n$ and the corresponding labels $Y = y_1, \dots, y_n$, the bagging algorithm repeats for $B$ times the following process:
\begin{enumerate}
	\item The algorithm selects a random sample with replacement of the training set $X_b$, $Y_b$
	\item The model $f_b$ is then trained on the $X_b$,$Y_b$ sets.
\end{enumerate}


The predictions for unseen samples $x^\prime$ are calculated by taking the primary vote of each model $f_b$.
The parameter $B$ is free, and it can go from a few hundred to several thousand, depending on the training set size. The optimal value can be found via cross-validation, or by examining the out-of-bag-error. \cite{james2013introduction}

\subsection{Bagging in Random Forest}
In a random forest, the bagging algorithm slightly differs from the one presented above. The algorithm selects a random subset of features, a process also known as "features bagging".

The reason of this change is the correlation of trees in an ordinary bootstrap sample. If few features are a powerful predictor for the output, then most of the $B$ trees select those features, causing the trees to be correlated. Ho gives an analysis of how bagging and random subspace projection contribute to the accuracy of the model. \cite{ho2002data}

Commonly, in a classification problem with $p$ features, the model uses $\sqrt{p}$ features at each split. Those parameters depend on the problem, and they should be treated as tuning parameters. \cite{hastie2009elements}

\subsection{Features importance}
Random forest ranks the features based on their importance to the model. Breiman describes this technique in his paper. \cite{breiman2001random}

The first step is to fit the model to the data. During this process, the model calculates the out-of-bag-error for each feature and records it. The model determines the importance of the $i^{th}$ feature by permutating the value of the $i^{th}$ feature against the training set, and then it calculates the out-of-bag-error again. 

The average of the difference in out-of-bag-error before and after the permutations, normalized by the standard deviation of these differences, represents the feature's importance score.

The higher is the score, the more important is the feature for the model.

However, this method does not work correctly with categorical variables. If these features have different levels, the model is more likely to bias the one with more levels. Using partial permutations and growing unbiased trees, it is possible to reduce these problems. \cite{deng2011bias}

\section{XGBoost}
XGBoost stands for e\textbf{X}treme \textbf{G}radient \textbf{Boos}ting. It is an open-source optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. 

XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples. \cite{xgboost}

The XGBoost model supports three different forms of gradient boosting: \cite{gentleXgboost}
\begin{itemize}
	\item \textbf{Gradient Boosting} with learning rate
	\item \textbf{Stochastic Gradient Boosting} with sub-sampling at the row, column and column per split levels.
	\item \textbf{Regularized Gradient Boosting} with both L1 and L2 regularization
\end{itemize} 

\subsection{Gradient Boosting}

Gradient boosting is a machine learning technique for regression and classification, which produces a prediction model in the form of an ensemble of weak prediction models. 

Gradient Boosting is a modified version of the Boosting algorithm.
Boosting is an ensemble method that converts weak learners into strong ones. It adds new models to compensate the shortcomings of by existing models until the error can not be reduced anymore. \cite{zhou2012ensemble}

Gradient Boosting is a boosting algorithm that uses a \textit{gradient descent algorithm} to minimize the error when adding new models.\\

Suppose we want to train a model $F$ to predict some values $y = F(x)$. At each step $m$ we have a weak model $F_m$. To improve the model $F_m$, the gradient descent algorithm creates a new model $F_{m+1}$ by adding to the previous model an estimator $h$ such that $F_{m+1}(x) = F_m(x) + h(x)$. \cite{gradientBoostLi}

To find $h$ the gradient descent algorithm starts from the perfect solution where $F_{m+1}(x) = F_m(x) + h(x) = y$, i.e. $h(x) = y - F_m(x)$. Consequently gradient boosting will fit $h$ to the residual $y - F_m(x)$.
\section{SVC}
Support Vector Machine
\section{Cross-validation}
\subsection{KFold}

\section{Features Selection}
\subsection{Filter method}
\subsection{Wrapped method}
\subsection{Embedded method}
\chapter{Features Creation}

When we decided which features could be the most representative for our model, we choose to use only static features. We are looking for a framework fast and efficient, that can analyze lots of sample without being resource expensive.

At first, we tried to replicate the work done by Caliskan et al., but we found that most of the tools used depend on software no more maintained. Some of those tools do not work as expected, and others were slow in processing files. To simplify as much as possible the process of analyzing executables, we decided to use only Ghidra as software for extracting features.\\


Ghidra comes with a headless analyzer, which analyzes and runs scripts on the given sample. The headless version can run in any server, even without a desktop environment. So we built up a virtual machine in the Sapienza network and installed Ghidra there.  Unfortunately, Ghidra's documentation is not exhaustive since it was released less than a year ago. The hardest part was understanding Ghidra's APIs and how to exploit them for our purpose. The already made scripts were useful for our task because they contain many approaches for extracting data. \\


\section{Disassemble features}

The extraction of disassembled code was an easy and fast task. The documentation provides all the information on how to correctly use the disassembler. We wrote a script that extracts the disassembled code for each function and stores it into a .dis file. The script creates a folder for each sample and stores inside the disassembled code.

From the disassembled code, we extracted 5 kinds of various features:
\begin{itemize}
	\item{Entire line unigrams}
	\item{Disassemble unigrams}
	\item{Disassemble bigrams}
	\item{Instruction only unigrams}
	\item{Instruction only bigrams}
\end{itemize}

First of all, we stripped out all the hexadecimal and numbers, replacing the regex '\d+' with the word 'number', and '0[xX][0-9a-fA-F]+' with the word hexadecimal. Stripping the numbers and hexadecimal reduced the possibility of overfitting because some numbers may be unique, and that would create a useless feature.

Furthermore, we create a .csv file for each sample, containing all the features calculated, the md5, used as an identifier of the executable, and the apt name. Then all the files are merged into a big .h5 with all the samples. 
In the first approach, we stored all the features into a .csv file, but the more features we extract, the more significant were the dimensionality of our dataset. When it comes to reading into python, pandas was very slow in both reading and processing the files.
A valid alternative to pandas is Dask, a flexible parallel computing library for analytics, that integrates with pandas,  numpy, and scikit. However, the dask-ml package lacks some functionalities for the cross-validation and random forest model. Furthermore, It was still slow in reading bigger files, so we decided to find another solution to speed up the process.
In the end, we decided to store our dataset into a Hierarchical Data Format (HDF5) designed to store and organize large amounts of data. This format comes with a cost, the files are much bigger, but we drastically improved the speed of reading and processing the dataset.

\subsection{Entire line unigram}
The first block of features is the whole line unigram, we split the disassembled code of each function on the new-line character and then count all the occurrences of different line instructions. We stripped out all the commas because, in the beginning, we saved the dataset to .csv with comma as a separator. For example, the features of the following disassembled function would be:

\begin{table}[!htb]
\begin{minipage}{.5\linewidth}
	\centering
	
	\caption{Code for function f}
	\label{tab:function_f}
	
	\medskip
	
	\begin{tabular}{ l } 
		\toprule
		push ebx \\
		mov eax, 1\\
		cmp ebx, eax\\
		jle 0xDEADBEEF\\
		add eax, 1\\
		cmp ebx, eax\\
		jle 0xBACADDAC\\
		mov eax, 0x400231BC\\
		call eax\\
		ret\\
	
		
		\bottomrule
	\end{tabular}
\end{minipage}\hfill
\begin{minipage}{.5\linewidth}
	\centering
	
	\caption{Entire line unigrams}
	\label{tab:line_unigrams}
	
	\medskip
	
	\begin{tabular}{  lr } 
		\toprule
		\makecell{ Feature }  &  Value \\   
		
		\midrule push ebx       & 1         \\
		mov eax,number & 1                  \\ 
		cmp ebx,eax    & 2                  \\ 
		add ebx, number     & 2                  \\ 
		jle hex        & 2                  \\
		mov eax, hexadecimal & 1\\ 
		call eax       & 1                  \\
		ret & 1\\ 
		apt            & PatchWork          \\ 
		md5            & 1234dc...eb121 \\ 
		\bottomrule
	\end{tabular}
\end{minipage}
\end{table}


\subsection{Disassemble unigrams and bigrams}
For this block of features, we split the entire line in instruction, eventual registers, or numbers. We first divided on the first space, and then if the second half of the string still contains data, we split for all the commas to get the single registers/number. the line "mov eax, 0x12" would be split in the following array: ["mov", "eax", "hexadecimal"] . As before, we counted the occurrences of every word in the file. 

For the unigram files, we only considered as a feature every word we would obtain after splitting the string. For the bigram files, instead, we considered as a feature the pair of words in the file. 

Furthermore, we added a start token ("<s>")at the beginning of the function file, and an end token ("</s>") at the end of the file. We concatenate the first and second element of the bigram with the the string "=>" The features generated from the same disassembled code would be the following: 

\begin{table}[!htb]
	\begin{minipage}{.5\linewidth}
		\centering
		
		\caption{Disassemble unigrams}
		\label{tab:dis_uni}
		
		\medskip
		
		\begin{tabular}{  lr } 
			\toprule
			\makecell{ Feature }  &  Value \\   
			
			\midrule 
			push & 1	\\
			ebx & 3\\
			mov & 2\\
			eax & 6\\
			number & 2\\
			cmp & 2\\
			jle & 2\\
			hex & 3\\
			add & 1\\
			call & 1\\
			ret & 1\\
			apt            & PatchWork          \\ 
			md5            & 1234dc...eb121 \\ 
			\bottomrule
		\end{tabular}
	\end{minipage}\hfill
	\begin{minipage}{.5\linewidth}
		\centering
		
		\caption{Disassemble bigrams}
		\label{tab:dis_big}
		
		\medskip
		
		\begin{tabular}{  lr } 
			\toprule
			\makecell{ Feature }  &  Value \\   
			
			\midrule 
			<s>=>push & 1	\\
			push=>ebx & 1\\
			ebx=>mov & 1\\
			mov=>eax & 2\\
			eax=>num & 2\\
			num=>cmp & 2\\
			cmp=>ebx & 2\\
			ebx=>eax & 2\\
			eax=>jle & 2\\
			jle=>hex & 2\\
			hex=>add & 1\\
			add=>eax & 1\\
			hex=>mov & 1\\
			eax=>hex & 1\\
			hex=>call & 1\\
			call=>hex & 1\\
			hex=>ret & 1\\
			ret=></s> & 1\\
			apt            & PatchWork          \\ 
			md5            & 1234dc...eb121 \\ 
			\bottomrule
		\end{tabular}
	\end{minipage}
\end{table}


\subsection{Instruction only unigrams and bigrams}
For the last block of features, we decided to study only the frequency of the different instructions in the code, without considering the registry. As before in the bigrams, we added a start and an end token to avoid linking two instructions from different functions. The features from the previous example would be:

\begin{table}[!htb]
	\begin{minipage}{.5\linewidth}
		\centering
		
		\caption{Instruction only unigrams}
		\label{tab:instr_uni}
		
		\medskip
		
		\begin{tabular}{  lr } 
			\toprule
			\makecell{ Feature }  &  Value \\   
			
			\midrule 
			push & 1	\\
			mov & 2\\
			cmp & 2\\
			jle & 2\\
			add & 1\\
			call & 1\\
			ret & 1\\
			apt            & PatchWork          \\ 
			md5            & 1234dc...eb121 \\ 
			\bottomrule
		\end{tabular}
	\end{minipage}\hfill
	\begin{minipage}{.5\linewidth}
		\centering
		
		\caption{Instruction only bigrams}
		\label{tab:instr_big}
		
		\medskip
		
		\begin{tabular}{  lr } 
			\toprule
			\makecell{ Feature }  &  Value \\   
			
			\midrule 
			<s>=>push & 1	\\
			push=>mov & 1\\
			mov=>cmp & 1\\
			cmp=>jle & 2\\
			jle=>add & 1\\
			add=>cmp & 1\\
			jle=>mov & 1\\
			mov=>call & 1\\
			call=>ret & 1\\
			ret=></s> & 1\\
			apt            & PatchWork          \\ 
			md5            & 1234dc...eb121 \\ 
			\bottomrule
		\end{tabular}
	\end{minipage}
\end{table}

\section{Control Flow Graph features}

\textbf{la parte che segue è tutta da riscrivere in quanto CTRL+C CTRL+V diretto da Ghidra doc} \cite{ghidra_pcode}
\subsection{PCode}
P-code is a register transfer language designed for reverse engineering applications. The language is general enough to model the behavior of many different processors. By modeling in this way, the analysis of different processors is put into a common framework, facilitating the development of retargetable analysis algorithms and applications.

Fundamentally, p-code works by translating individual processor instructions into a sequence of p-code operations that take parts of the processor state as input and output variables (varnodes). The set of unique p-code operations (distinguished by opcode) comprise a fairly tight set of the arithmetic and logical actions performed by general purpose processors. The direct translation of instructions into these operations is referred to as raw p-code. Raw p-code can be used to directly emulate instruction execution and generally follows the same control-flow, although it may add some of its own internal control-flow. The subset of opcodes that can occur in raw p-code is described in the section called “P-Code Operation Reference” and in the section called “Pseudo P-CODE Operations”, making up the bulk of this document.

P-code is designed specifically to facilitate the construction of data-flow graphs for follow-on analysis of disassembled instructions. Varnodes and p-code operators can be thought of explicitly as nodes in these graphs. Generation of raw p-code is a necessary first step in graph construction, but additional steps are required, which introduces some new opcodes. Two of these, MULTIEQUAL and INDIRECT, are specific to the graph construction process, but other opcodes can be introduced during subsequent analysis and transformation of a graph and help hold recovered data-type relationships. All of the new opcodes are described in the section called “Additional P-CODE Operations”, none of which can occur in the original raw p-code translation. Finally, a few of the p-code operators, CALL, CALLIND, and RETURN, may have their input and output varnodes changed during analysis so that they no longer match their raw p-code form.
\subsection{Address Space}
The address space for p-code is a generalization of RAM. It is defined simply as an indexed sequence of bytes that can be read and written by the p-code operations. For a specific byte, the unique index that labels it is the byte's address. An address space has a name to identify it, a size that indicates the number of distinct indices into the space, and an endianess associated with it that indicates how integers and other multi-byte values are encoded into the space. A typical processor will have a ram space, to model memory accessible via its main data bus, and a register space for modeling the processor's general purpose registers. Any data that a processor manipulates must be in some address space. The specification for a processor is free to define as many address spaces as it needs. There is always a special address space, called a constant address space, which is used to encode any constant values needed for p-code operations. Systems generating p-code also generally use a dedicated temporary space, which can be viewed as a bottomless source of temporary registers. These are used to hold intermediate values when modeling instruction behavior.

P-code specifications allow the addressable unit of an address space to be bigger than just a byte. Each address space has a wordsize attribute that can be set to indicate the number of bytes in a unit. A wordsize which is bigger than one makes little difference to the representation of p-code. All the offsets into an address space are still represented internally as a byte offset. The only exceptions are the LOAD and STORE p-code operations. These operations read a pointer offset that must be scaled properly to get the right byte offset when dereferencing the pointer. The wordsize attribute has no effect on any of the other p-code operations. 

\subsection{Varnode}
A varnode is a generalization of either a register or a memory location. It is represented by the formal triple: an address space, an offset into the space, and a size. Intuitively, a varnode is a contiguous sequence of bytes in some address space that can be treated as a single value. All manipulation of data by p-code operations occurs on varnodes.

Varnodes by themselves are just a contiguous chunk of bytes, identified by their address and size, and they have no type. The p-code operations however can force one of three type interpretations on the varnodes: integer, boolean, and floating-point.

Operations that manipulate integers always interpret a varnode as a twos-complement encoding using the endianess associated with the address space containing the varnode.
A varnode being used as a boolean value is assumed to be a single byte that can only take the value 0, for false, and 1, for true.
Floating-point operations use the encoding expected by the processor being modeled, which varies depending on the size of the varnode. For most processors, these encodings are described by the IEEE 754 standard, but other encodings are possible in principle.

If a varnode is specified as an offset into the constant address space, that offset is interpreted as a constant, or immediate value, in any p-code operation that uses that varnode. The size of the varnode, in this case, can be treated as the size or precision available for the encoding of the constant. As with other varnodes, constants only have a type forced on them by the p-code operations that use them. 

\subsection{Pcode Operations}
A p-code operation is the analog of a machine instruction. All p-code operations have the same basic format internally. They all take one or more varnodes as input and optionally produce a single output varnode. The action of the operation is determined by its opcode. For almost all p-code operations, only the output varnode can have its value modified; there are no indirect effects of the operation. The only possible exceptions are pseudo operations, see the section called “Pseudo P-CODE Operations”, which are sometimes necessary when there is incomplete knowledge of an instruction's behavior.

All p-code operations are associated with the address of the original processor instruction they were translated from. For a single instruction, a 1-up counter, starting at zero, is used to enumerate the multiple p-code operations involved in its translation. The address and counter as a pair are referred to as the p-code op's unique sequence number. Control-flow of p-code operations generally follows sequence number order. When execution of all p-code for one instruction is completed, if the instruction has fall-through semantics, p-code control-flow picks up with the first p-code operation in sequence corresponding to the instruction at the fall-through address. Similarly, if a p-code operation results in a control-flow branch, the first p-code operation in sequence executes at the destination address.

\subsection{Control Flow Graph}
cos'è e cosa fa

\subsection{What we did}
titolo da cambiare\\

We rely on Ghidra's Pcode representation to build our dataset for Control Flow Graph. Ghidra contains three different scripts for analyzing the flow of the program, and we studied those scripts to understand how Ghidra manages the Pcode and their flow. The script iterates all the functions of the given sample and generates a .json file with the extracted data.\\

Ghidra offers a DecompileInterface, a class that can decompile a function, and that returns an object DecompiledResult with all the information needed. It is also possible to pass different options to the DecompileInterface using the DecompileOption class. The resulting object contains an instance of HighFunction, a high-level abstraction associated with a low-level function made up of assembly instructions. The HighFunction object offers the possibility to iterate over the BasicBlocks of the corresponding function so we can analyze all the blocks and create our graph.\\

The graph is composed of an array of basic blocks, each of which has an index, a list of pcodes, and two lists, one containing the indexes of the previous basic blocks and the other one the indexes where the basic block points, i.e., the flow of our function. The pcodes have a field with the associated pcode operation, a list of input varnodes, and a possible output varnode. \\

The main problem encountered running the script, is the decompilation time. Some functions were intricate, and when it comes to decompile, Ghidra can take a very long time, even 25 minutes per sample.  Furthermore, the DecompileOption has a field indicating the maximum dimension of the payload of the decompiled function. The default value is 50MB, but for some specific functions, it is still low, and we needed to increase it to 500MB correctly decompile all the functions.  \\

From the CFG files, we extracted 3 kinds of features:
\begin{itemize}
	\item {Control Flow Graph unigrams complete}
	\item {Control Flow Graph unigrams Pcode only}
	\item {Control Flow Graph bigrams Pcode only}
\end{itemize}

\subsection{Control Floe Graph unigrams complete}
This first set of features contains the unigrams of the complete Pcode representation. The key for each feature is the concatenation of the Pcode, the input and output nodes. In particular, we construct the key as follow:
\texttt{PCODE\_nodeoutput\#nodeinput*count} of nodes
So this .json file  is converted to:

\texttt{"pcodes": [
\{
	"code": "CALL",
	"varnode\_in": [
	"ram",
	"const"
	],
	"count": 2
\}]}
\texttt{key = call\_ram\#const*2}
\textbf{**Sistemare qua**}

We counted the occurrences of each key and build our dataset.

\subsection{Control Flow Graph Pcode only unigrams and bigrams}

These two sets of features contain the unigrams and bigrams of the pcode only. We built the key using only the pcode operator, and then counted the occurrences. For the bigrams, we concatenated as before the key with the string =>.

\subsection{Cyclomatic Complexity}
Cyclomatic complexity is a software metric used to indicate the complexity of a program. Cyclomatic complexity is computed using the control flow graph of the program: the nodes of the graph correspond to indivisible groups of commands of a program, and a directed edge connects two nodes if the second command might be executed immediately after the first command. Cyclomatic complexity may also be applied to individual functions, modules, methods, or classes within a program. The cyclomatic complexity of a section of source code is the number of linearly independent paths within it. For instance, if the source code contained no control flow statements (conditionals or decision points), the complexity would be 1, since there would be only a single path through the code. If the code had one single-condition IF statement, there would be two paths through the code: one where the IF statement evaluates to TRUE and another one where it evaluates to FALSE so that the complexity would be 2. Two nested single-condition IFs, or one IF with two conditions, would produce a complexity of 3.
Mathematically, the cyclomatic complexity of a structured program[a] is defined regarding the control flow graph of the program, a directed graph containing the basic blocks of the program, with an edge between two basic blocks if control may pass from the first to the second. The complexity M is then defined as[2]
M = E - N + 2P,
where
E = the number of edges of the graph.
N = the number of nodes of the graph.
P = the number of connected components.

The same function as above, represented using the alternative formulation, where each exit point is connected back to the entry point. This graph has 10 edges, 8 nodes, and 1 connected component, which also results in a cyclomatic complexity of 3 using the alternative formulation (10 - 8 + 1 = 3).
An alternative formulation is to use a graph in which each exit point is connected back to the entry point. In this case, the graph is strongly connected, and the cyclomatic complexity of the program is equal to the cyclomatic number of its graph (also known as the first Betti number), which is defined as[2]
M = E - N + P.
This may be seen as calculating the number of linearly independent cycles that exist in the graph, i.e., those cycles that do not contain other cycles within themselves. Note that because each exit point loops back to the entry point, there is at least one such cycle for each exit point.
For a single program (or subroutine or method), P is always equal to 1. So a simpler formula for a single subroutine is
M = E - N + 2
Cyclomatic complexity may, however, be applied to several such programs or subprograms at the same time (e.g., to all of the methods in a class), and in these cases, P will be equal to the number of programs in question, as each subprogram will appear as a disconnected subset of the graph.
McCabe showed that the cyclomatic complexity of any structured program with only one entry point and one exit point is equal to the number of decision points (i.e., "if" statements or conditional loops) contained in that program plus one. However, this is true only for decision points counted at the lowest, machine-level instructions.[4] Decisions involving compound predicates like those found in high-level languages like IF cond1 AND cond2 THEN ... should be counted in terms of predicate variables involved, i.e., in this example, one should count two decision points, because at machine level it is equivalent to IF cond1 THEN IF cond2 THEN [2][5]
Cyclomatic complexity may be extended to a program with multiple exit points; in this case, it is equal to
pi - s + 2,
where pi is the number of decision points in the program, and s is the number of exit points.\cite{complexity}  \textbf{Citato da wikipedia, da modificare tutto}\\

Ghidra offers a class to compute the complexity of a function, CyclomaticCompelxity. This class has a method to calculate the cyclomatic complexity of a function by decomposing it into a flow graph using a BasicBlockModel. During the decompilation, we calculate the complexity of each function and stores it into the .json file. Then we calculate as a feature, the mean, the standard deviation, and the maximum value of complexity for each sample.

\subsection{Standard Library}

One primary task of reverse engineering binary code is to identify library code. Since what the library code does is often known, it is of no interest to an analyst. Hex-Rays has developed the IDA FLIRT signatures to tackle the problem. Function ID is Ghidra's function signature system. Unfortunately, Ghidra has very few Function ID datasets. There is only function identification for the Visual Studio supplied libraries. Ghidra's Function ID allows identifying functions based on hashing the masked function bytes automatically.\cite{ghidra_fid}\\

We exploit this functionality to determine which of the functions belongs to a standard library. Then we calculate the number of standard functions in the given sample and use it as a feature.

\section{Rich Header features}
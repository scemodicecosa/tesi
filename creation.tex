\chapter{Features Creation}

When we decided which features could be the most representative for our model, we choose to use only static features. Extracting only static features usually do not require too much time, and there is no need for expensive virtualized systems. In the dynamic analysis, the sample has to be executed in a controlled environment to analyze its behavior. 

Since we want to boost up the work made by Laurenza et al., we choose to continue their path of using only static features, but we added novel features from code analysis.  We are looking for a framework fast and efficient, that can analyze lots of sample without being resource expensive.

At first, we tried to replicate the work done by Caliskan et al., but we found that most of the tools used depend on software no more maintained. Some of those tools do not work as expected, and others were slow in processing files. To simplify as much as possible the process of analyzing executables, we decided to use only Ghidra as software for extracting features.

The following sections cover the blocks of features extracted, explaining the procedure used to accomplish this task.

\section{Dataset file format}
The choice of the format used for storing our dataset was crucial.
We decided to save the feature vector of each sample into a \texttt{.csv } file. Using the md5 we found the related APT campaign in the dAPTaset. Each \texttt{.csv} file contains a column named md5, a column name apt, and \texttt{n} columns corresponding to the features vector of the sample. During the classification phase the md5s are stripped out from the dataset and the apt column is the label of each sample.

To generate the dataset with the feature vectors of all the samples, we tried different approaches. 

The first one was to merge all the \texttt{.csv} files into a single big \texttt{.csv}. Unfortunately, the more features we extracted, the more significant was the dimensionality of our dataset.  When it comes to reading into python, pandas was very slow in both reading and processing the files.

A valid alternative to pandas is \textit{Dask} \cite{dask}, a flexible parallel computing library for analytics, that integrates with \textit{pandas},  \textit{numpy}, and \textit{scikit}. However, the dask-ml package lacks some functionalities for the cross-validation and random forest model. Furthermore, It was still slow in reading bigger files, so we decided to find another solution to speed up the process.

In the end, we decided to store our dataset into a \textit{Hierarchical Data Format} (\textbf{HDF5}) \cite{hdf} designed to store and organize large amounts of data. This format comes with a cost, the files are much bigger, but we drastically improved the speed of reading and processing the dataset.


\section{Disassembled features}

The extraction of disassembled code was an easy and fast task. The documentation provides all the information on how to correctly use the disassembler. We wrote a script that extracts the disassembled code for each function and stores it into a \texttt{.dis} file. The script creates a folder for each sample and stores inside the disassembled code.

From the disassembled code, we extracted 5 kinds of features:
\begin{itemize}
	\item{Line unigrams}
	\item{Disassemble unigrams}
	\item{Disassemble bigrams}
	\item{Instruction only unigrams}
	\item{Instruction only bigrams}
\end{itemize}

First of all, we stripped out all the hexadecimal and numbers, replacing the regex \texttt{"$\backslash$d+"} with the word \texttt{"number"}, and \texttt{"0[xX][0-9a-fA-F]+"} with the word \texttt{"hexadecimal"}. Stripping the numbers and hexadecimal reduced the possibility of overfitting because some numbers may be unique, and that would create a useless feature.


\subsection{Line unigram}
The first block of features is the whole line unigram, we split the disassembled code of each function on the new-line character and then count all the occurrences of different line instructions. We stripped out all the commas because, in the beginning, we saved the dataset to \texttt{.csv} with comma as a separator. For example, the features generated from the function f in Table \ref{tab:function_f} are shown in Table \ref{tab:line_unigrams}

\begin{table}[!htb]
\begin{minipage}{.5\linewidth}
	\centering
	
	\caption{Assembly code of function f}
	\label{tab:function_f}
	
	\medskip
	
	\begin{tabular}{ l } 
		\toprule
		\texttt{push ebx} \\
		\texttt{mov eax, 1}\\
		\texttt{cmp ebx, eax}\\
		\texttt{jle 0xDEADBEEF}\\
		\texttt{add eax, 1}\\
		\texttt{cmp ebx, eax}\\
		\texttt{jle 0xBACADDAC}\\
		\texttt{mov eax, 0x400231BC}\\
		\texttt{call eax}\\
		\texttt{ret}\\
	
		
		\bottomrule
	\end{tabular}
\end{minipage}\hfill
\begin{minipage}{.5\linewidth}
	\centering
	
	\caption{Feature vector of line unigrams}
	\label{tab:line_unigrams}
	
	\medskip
	
	\begin{tabular}{  lr } 
		\toprule
		\makecell{ Feature }  &  Value \\   
		
		\midrule 
		\texttt{push ebx} & 1         \\
		\texttt{mov eax,number} & 1                  \\ 
		\texttt{cmp ebx,eax }   & 2                  \\ 
		\texttt{add ebx, number}     & 2                  \\ 
		\texttt{jle hex }       & 2                  \\
		\texttt{mov eax, hexadecimal} & 1\\ 
		\texttt{call eax}       & 1                  \\
		\texttt{ret} & 1\\
		\bottomrule
	\end{tabular}
\end{minipage}
\end{table}


\subsection{Disassemble unigrams and bigrams}
For this block of features, we split the entire line in instruction, eventual registers, or numbers. Firstly, we split the line on the first space, then if the second half of the string still contains data, we split for all the commas to get the single registers/numbers. The line \texttt{"mov eax, 0x12"} would be split in the following array: \texttt{["mov", "eax", "hexadecimal"]} . As before, we counted the occurrences of every word in the file. 

For the unigram files, we only considered as a feature every word we would obtain after splitting the string. For the bigram files, instead, we considered as a feature the pair of words in the file. 

Furthermore, we added a start token (\texttt{"<s>"}) before the first instruction of every function, and an end token (\texttt{"</s>"}) after the last instruction. We concatenate the first and second element of the bigram with the the string \texttt{"=>"} The features generated from the same disassembled code are presented in Table \ref{tab:dis_uni} and \ref{tab:dis_big}.

\begin{table}[!htb]
	\begin{minipage}{.5\linewidth}
		\centering
		
		\caption{Feature vector of disassemble unigrams}
		\label{tab:dis_uni}
		
		\medskip
		
		\begin{tabular}{  lr } 
			\toprule
			\makecell{ Feature }  &  Value \\   
			
			\midrule 
			\texttt{push} & 1	\\
			\texttt{ebx} & 3\\
			\texttt{mov} & 2\\
			\texttt{eax} & 6\\
			\texttt{number} & 2\\
			\texttt{cmp} & 2\\
			\texttt{jle} & 2\\
			\texttt{hex} & 3\\
			\texttt{add} & 1\\
			\texttt{call} & 1\\
			\texttt{ret} & 1\\
			\bottomrule
		\end{tabular}
	\end{minipage}\hfill
	\begin{minipage}{.5\linewidth}
		\centering
		
		\caption{Feature vector of disassemble bigrams}
		\label{tab:dis_big}
		
		\medskip
		
		\begin{tabular}{  lr } 
			\toprule
			\makecell{ Feature }  &  Value \\   
			
			\midrule 
			\texttt{<s>=>push} & 1	\\
			\texttt{push=>ebx} & 1\\
			\texttt{ebx=>mov} & 1\\
			\texttt{mov=>eax} & 2\\
			\texttt{eax=>num} & 2\\
			\texttt{num=>cmp} & 2\\
			\texttt{cmp=>ebx} & 2\\
			\texttt{ebx=>eax} & 2\\
			\texttt{eax=>jle} & 2\\
			\texttt{jle=>hex} & 2\\
			\texttt{hex=>add} & 1\\
			\texttt{add=>eax} & 1\\
			\texttt{hex=>mov} & 1\\
			\texttt{eax=>hex} & 1\\
			\texttt{hex=>call} & 1\\
			\texttt{call=>hex} & 1\\
			\texttt{hex=>ret} & 1\\
			\texttt{ret=></s>} & 1\\
			\bottomrule
		\end{tabular}
	\end{minipage}
\end{table}


\subsection{Instruction only unigrams and bigrams}
For the last block of features, we decided to study only the frequency of the different instructions in the code, without considering the registry. As before in the bigrams, we added a start and an end token to avoid linking two instructions from different functions. Tables \ref{tab:instr_uni} and \ref{tab:instr_big} show the features generated from the previous example.

\begin{table}[!htb]
	\begin{minipage}{.5\linewidth}
		\centering
		
		\caption{Feature vector of instruction only unigrams}
		\label{tab:instr_uni}
		
		\medskip
		
		\begin{tabular}{  lr } 
			\toprule
			\makecell{ Feature }  &  Value \\   
			
			\midrule 
			\texttt{push} & 1	\\
			\texttt{mov} & 2\\
			\texttt{cmp} & 2\\
			\texttt{jle} & 2\\
			\texttt{add} & 1\\
			\texttt{call} & 1\\
			\texttt{ret} & 1\\
			\bottomrule
		\end{tabular}
	\end{minipage}\hfill
	\begin{minipage}{.5\linewidth}
		\centering
		
		\caption{Feature vector of instruction only bigrams}
		\label{tab:instr_big}
		
		\medskip
		
		\begin{tabular}{  lr } 
			\toprule
			\makecell{ Feature }  &  Value \\   
			
			\midrule 
			\texttt{<s>=>push} & 1	\\
			\texttt{push=>mov} & 1\\
			\texttt{mov=>cmp} & 1\\
			\texttt{cmp=>jle} & 2\\
			\texttt{jle=>add} & 1\\
			\texttt{add=>cmp} & 1\\
			\texttt{jle=>mov} & 1\\
			\texttt{mov=>call} & 1\\
			\texttt{call=>ret} & 1\\
			\texttt{ret=></s>} & 1\\
			\bottomrule
		\end{tabular}
	\end{minipage}
\end{table}

\section{Control Flow Graph features}
For \textit{Control Flow Graph} features we proceed by creating a \texttt{.json} file with all the information gathered from the Decompiler as explained in Subsection \ref{subsec:cfg_ghidra}.

From the CFG files, we extracted three kinds of features:
\begin{itemize}
	\item {Control Flow Graph unigrams complete}
	\item {Control Flow Graph unigrams Pcode only}
	\item {Control Flow Graph bigrams Pcode only}
\end{itemize}

\subsection{Control Flow Graph unigrams complete}
This first set of features contains the unigrams of the complete Pcode representation.
For each sample we extract, from the corresponding \texttt{.json} file, all the pcodes in that function. We consider as a feature every unique combination of pcodes with input and output varnodes.
The key for each feature is the concatenation of the PcodeOP, the input and output varnodes. In particular, we construct the key as follow:
\texttt{PCodeOP\_nodeoutput\#nodeinput*count} of nodes.
The key of the example in \ref{lst:jsonpcode} is \texttt{call\_ram\#const*2}. As before we counted the occurrences of each key and we built our dataset.

\begin{lstlisting}[language=json,firstnumber=1,caption={Example of .json format with PCode},label={lst:jsonpcode}]
{
"pcodes": [
	{
	"code": "CALL",
	"varnode_in": ["ram","const"],
	"count": 2 
	}]
}
\end{lstlisting}



\subsection{Control Flow Graph Pcode only unigrams and bigrams}

These two sets of features contain the unigrams and bigrams of the pcode only. We built the key using only the pcode operator, and then counted the occurrences. For the bigrams, we concatenated as before the key with the string \texttt{"=>"}.

\subsection{Cyclomatic Complexity}

To calculate the Cyclomatic Complexity of a given function, we used the class from Ghidra's API \texttt{CyclomaticComplexity}. The function has a method that returns the complexity of the given function. We saved for every function of each sample its complexity. 

Then, for each sample, we calculated \textit{maximum}, \textit{mean}, and \textit{standard variation} of the complexity of all the functions, and used those as features.

\subsection{Standard Library}

One primary task of reverse engineering binary code is to identify library code. Since what the library code does is often known, it is of no interest to an analyst. Hex-Rays has developed the IDA FLIRT signatures to tackle the problem. 

Function ID is Ghidra's function signature system. Unfortunately, Ghidra has very few Function ID datasets. There is only function identification for the Visual Studio supplied libraries. Ghidra's Function ID allows identifying functions based on hashing the masked function bytes automatically\cite{ghidra_fid}.

We exploit this functionality to determine which of the functions belongs to a standard library and added a boolean field in the \texttt{.json} file, indicating whether that function is or not a standard known function. Then we calculated the number of standard functions in the given sample and use it as a feature.

\section{Rich Header features}
We used the script in paper \cite{dubyk2019sans} to calculate the rich header hash of each sample. Sadly, as pointed out in the paper, not every binary is compiled with the rich header; in fact, only 1669 samples out of 2086 have it. 

The script extracts the \texttt{productID}, the \texttt{productVersion}, and \texttt{productCount}, we concatenated those numbers with a dash \texttt{"-"} to create a key and set 1 if the sample contains the previous key, 0 otherwise.

\section{Total features}
Table \ref{tab:num_feat} shows the number of features of each type, with a total number of \textbf{218012} features.

\begin{table}[!htb]
		\centering
		\caption{Number of features of each type}
		\label{tab:num_feat}
			\begin{tabular}{ll}
				\toprule
				Features type                    & Dimensionality \\
				\midrule
				Disassemble unigrams             & 2476           \\ 
				Disassemble bigrams              & 39697          \\ 
				Disassemble line unigrams        & 25927          \\ 
				Disassemble instruction unigrams & 347            \\ 
				Disassemble instruction bigrams  & 8125           \\ 
				CFG unigrams                     & 13536          \\ 
				CFG bigrams                      & 118852         \\ 
				CFG code unigrams                & 61             \\ 
				CFG code bigrams                 & 2026           \\ 
				CFG complexity                   & 4              \\ 
				Standard library function        & 5577           \\ 
				Rich Header complete             & 1217           \\ 
				Rich Header without count        & 167            \\ 
				\bottomrule
			\end{tabular}
		
\end{table}


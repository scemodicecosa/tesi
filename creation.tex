\chapter{Features Creation}

When we decided which features could be the most representative for our model, we choose to use only static features. We are looking for a framework fast and efficient, that can analyze lots of sample without being resource expensive.

At first, we tried to replicate the work done by Caliskan et al., but we found that most of the tools used depend on software no more maintained. Some of those tools do not work as expected, and others were slow in processing files. To simplify as much as possible the process of analyzing executables, we decided to use only Ghidra as software for extracting features.\\


Ghidra comes with a headless analyzer, which analyzes and runs scripts on the given sample. The headless version can run in any server, even without a desktop environment. So we built up a virtual machine in the Sapienza network and installed Ghidra there.  Unfortunately, Ghidra's documentation is not exhaustive since it was released less than a year ago. The hardest part was understanding Ghidra's APIs and how to exploit them for our purpose. The already made scripts were useful for our task because they contain many approaches for extracting data. \\


\section{Disassemble features}

The extraction of disassembled code was an easy and fast task. The documentation provides all the information on how to correctly use the disassembler. We wrote a script that extracts the disassembled code for each function and stores it into a .dis file. The script creates a folder for each sample and stores inside the disassembled code.

From the disassembled code, we extracted 5 kinds of various features:
\begin{itemize}
	\item{Entire line unigrams}
	\item{Disassemble unigrams}
	\item{Disassemble bigrams}
	\item{Instruction only unigrams}
	\item{Instruction only bigrams}
\end{itemize}

First of all, we stripped out all the hexadecimal and numbers, replacing the regex '\d+' with the word 'number', and '0[xX][0-9a-fA-F]+' with the word hexadecimal. Stripping the numbers and hexadecimal reduced the possibility of overfitting because some numbers may be unique, and that would create a useless feature.

Furthermore, we create a .csv file for each sample, containing all the features calculated, the md5, used as an identifier of the executable, and the apt name. Then all the files are merged into a big .h5 with all the samples. 
In the first approach, we stored all the features into a .csv file, but the more features we extract, the more significant were the dimensionality of our dataset. When it comes to reading into python, pandas was very slow in both reading and processing the files.
A valid alternative to pandas is Dask, a flexible parallel computing library for analytics, that integrates with pandas,  numpy, and scikit. However, the dask-ml package lacks some functionalities for the cross-validation and random forest model. Furthermore, It was still slow in reading bigger files, so we decided to find another solution to speed up the process.
In the end, we decided to store our dataset into a Hierarchical Data Format (HDF5) designed to store and organize large amounts of data. This format comes with a cost, the files are much bigger, but we drastically improved the speed of reading and processing the dataset.

\subsection{Entire line unigram}
The first block of features is the whole line unigram, we split the disassembled code of each function on the new-line character and then count all the occurrences of different line instructions. We stripped out all the commas because, in the beginning, we saved the dataset to .csv with comma as a separator. For example, the features of the following disassembled function would be:

\begin{table}[!htb]
\begin{minipage}{.5\linewidth}
	\centering
	
	\caption{Code for function f}
	\label{tab:function_f}
	
	\medskip
	
	\begin{tabular}{ l } 
		\toprule
		push ebx \\
		mov eax, 1\\
		cmp ebx, eax\\
		jle 0xDEADBEEF\\
		add eax, 1\\
		cmp ebx, eax\\
		jle 0xBACADDAC\\
		mov eax, 0x400231BC\\
		call eax\\
		ret\\
	
		
		\bottomrule
	\end{tabular}
\end{minipage}\hfill
\begin{minipage}{.5\linewidth}
	\centering
	
	\caption{Entire line unigrams}
	\label{tab:line_unigrams}
	
	\medskip
	
	\begin{tabular}{  lr } 
		\toprule
		\makecell{ Feature }  &  Value \\   
		
		\midrule push ebx       & 1         \\
		mov eax,number & 1                  \\ 
		cmp ebx,eax    & 2                  \\ 
		add ebx, number     & 2                  \\ 
		jle hex        & 2                  \\
		mov eax, hexadecimal & 1\\ 
		call eax       & 1                  \\
		ret & 1\\ 
		apt            & PatchWork          \\ 
		md5            & 1234dc...eb121 \\ 
		\bottomrule
	\end{tabular}
\end{minipage}
\end{table}


\subsection{Disassemble unigrams and bigrams}
For this block of features, we split the entire line in instruction, eventual registers, or numbers. We first divided on the first space, and then if the second half of the string still contains data, we split for all the commas to get the single registers/number. the line "mov eax, 0x12" would be split in the following array: ["mov", "eax", "hexadecimal"] . As before, we counted the occurrences of every word in the file. 

For the unigram files, we only considered as a feature every word we would obtain after splitting the string. For the bigram files, instead, we considered as a feature the pair of words in the file. 

Furthermore, we added a start token ("<s>")at the beginning of the function file, and an end token ("</s>") at the end of the file. We concatenate the first and second element of the bigram with the the string "=>" The features generated from the same disassembled code would be the following: 

\begin{table}[!htb]
	\begin{minipage}{.5\linewidth}
		\centering
		
		\caption{Disassemble unigrams}
		\label{tab:dis_uni}
		
		\medskip
		
		\begin{tabular}{  lr } 
			\toprule
			\makecell{ Feature }  &  Value \\   
			
			\midrule 
			push & 1	\\
			ebx & 3\\
			mov & 2\\
			eax & 6\\
			number & 2\\
			cmp & 2\\
			jle & 2\\
			hex & 3\\
			add & 1\\
			call & 1\\
			ret & 1\\
			apt            & PatchWork          \\ 
			md5            & 1234dc...eb121 \\ 
			\bottomrule
		\end{tabular}
	\end{minipage}\hfill
	\begin{minipage}{.5\linewidth}
		\centering
		
		\caption{Disassemble bigrams}
		\label{tab:dis_big}
		
		\medskip
		
		\begin{tabular}{  lr } 
			\toprule
			\makecell{ Feature }  &  Value \\   
			
			\midrule 
			<s>=>push & 1	\\
			push=>ebx & 1\\
			ebx=>mov & 1\\
			mov=>eax & 2\\
			eax=>num & 2\\
			num=>cmp & 2\\
			cmp=>ebx & 2\\
			ebx=>eax & 2\\
			eax=>jle & 2\\
			jle=>hex & 2\\
			hex=>add & 1\\
			add=>eax & 1\\
			hex=>mov & 1\\
			eax=>hex & 1\\
			hex=>call & 1\\
			call=>hex & 1\\
			hex=>ret & 1\\
			ret=></s> & 1\\
			apt            & PatchWork          \\ 
			md5            & 1234dc...eb121 \\ 
			\bottomrule
		\end{tabular}
	\end{minipage}
\end{table}


\subsection{Instruction only unigrams and bigrams}
For the last block of features, we decided to study only the frequency of the different instructions in the code, without considering the registry. As before in the bigrams, we added a start and an end token to avoid linking two instructions from different functions. The features from the previous example would be:

\begin{table}[!htb]
	\begin{minipage}{.5\linewidth}
		\centering
		
		\caption{Instruction only unigrams}
		\label{tab:instr_uni}
		
		\medskip
		
		\begin{tabular}{  lr } 
			\toprule
			\makecell{ Feature }  &  Value \\   
			
			\midrule 
			push & 1	\\
			mov & 2\\
			cmp & 2\\
			jle & 2\\
			add & 1\\
			call & 1\\
			ret & 1\\
			apt            & PatchWork          \\ 
			md5            & 1234dc...eb121 \\ 
			\bottomrule
		\end{tabular}
	\end{minipage}\hfill
	\begin{minipage}{.5\linewidth}
		\centering
		
		\caption{Instruction only bigrams}
		\label{tab:instr_big}
		
		\medskip
		
		\begin{tabular}{  lr } 
			\toprule
			\makecell{ Feature }  &  Value \\   
			
			\midrule 
			<s>=>push & 1	\\
			push=>mov & 1\\
			mov=>cmp & 1\\
			cmp=>jle & 2\\
			jle=>add & 1\\
			add=>cmp & 1\\
			jle=>mov & 1\\
			mov=>call & 1\\
			call=>ret & 1\\
			ret=></s> & 1\\
			apt            & PatchWork          \\ 
			md5            & 1234dc...eb121 \\ 
			\bottomrule
		\end{tabular}
	\end{minipage}
\end{table}

\section{Control Flow Graph features}


We rely on Ghidra's Pcode representation to build our dataset for Control Flow Graph. Ghidra contains three different scripts for analyzing the flow of the program, and we studied those scripts to understand how Ghidra manages the Pcode and their flow. The script iterates all the functions of the given sample and generates a .json file with the extracted data.\\

Ghidra offers a DecompileInterface, a class that can decompile a function, and that returns an object DecompiledResult with all the information needed. It is also possible to pass different options to the DecompileInterface using the DecompileOption class. The resulting object contains an instance of HighFunction, a high-level abstraction associated with a low-level function made up of assembly instructions. The HighFunction object offers the possibility to iterate over the BasicBlocks of the corresponding function so we can analyze all the blocks and create our graph.\\

The graph is composed of an array of basic blocks, each of which has an index, a list of pcodes, and two lists, one containing the indexes of the previous basic blocks and the other one the indexes where the basic block points, i.e., the flow of our function. The pcodes have a field with the associated pcode operation, a list of input varnodes, and a possible output varnode. \\

The main problem encountered running the script, is the decompilation time. Some functions were intricate, and when it comes to decompile, Ghidra can take a very long time, even 25 minutes per sample.  Furthermore, the DecompileOption has a field indicating the maximum dimension of the payload of the decompiled function. The default value is 50MB, but for some specific functions, it is still low, and we needed to increase it to 500MB correctly decompile all the functions.  \\

From the CFG files, we extracted 3 kinds of features:
\begin{itemize}
	\item {Control Flow Graph unigrams complete}
	\item {Control Flow Graph unigrams Pcode only}
	\item {Control Flow Graph bigrams Pcode only}
\end{itemize}

\subsection{Control Floe Graph unigrams complete}
This first set of features contains the unigrams of the complete Pcode representation. The key for each feature is the concatenation of the Pcode, the input and output nodes. In particular, we construct the key as follow:
\texttt{PCODE\_nodeoutput\#nodeinput*count} of nodes
So this .json file  is converted to:

\texttt{"pcodes": [
\{
	"code": "CALL",
	"varnode\_in": [
	"ram",
	"const"
	],
	"count": 2
\}]}
\texttt{key = call\_ram\#const*2}
\textbf{**Sistemare qua**}

We counted the occurrences of each key and build our dataset.

\subsection{Control Flow Graph Pcode only unigrams and bigrams}

These two sets of features contain the unigrams and bigrams of the pcode only. We built the key using only the pcode operator, and then counted the occurrences. For the bigrams, we concatenated as before the key with the string =>.

\subsection{Cyclomatic Complexity}
Cyclomatic complexity is a software metric used to indicate the complexity of a program. Cyclomatic complexity is computed using the control flow graph of the program: the nodes of the graph correspond to indivisible groups of commands of a program, and a directed edge connects two nodes if the second command might be executed immediately after the first command. Cyclomatic complexity may also be applied to individual functions, modules, methods, or classes within a program. The cyclomatic complexity of a section of source code is the number of linearly independent paths within it. For instance, if the source code contained no control flow statements (conditionals or decision points), the complexity would be 1, since there would be only a single path through the code. If the code had one single-condition IF statement, there would be two paths through the code: one where the IF statement evaluates to TRUE and another one where it evaluates to FALSE so that the complexity would be 2. Two nested single-condition IFs, or one IF with two conditions, would produce a complexity of 3.
Mathematically, the cyclomatic complexity of a structured program[a] is defined regarding the control flow graph of the program, a directed graph containing the basic blocks of the program, with an edge between two basic blocks if control may pass from the first to the second. The complexity M is then defined as[2]
M = E - N + 2P,
where
E = the number of edges of the graph.
N = the number of nodes of the graph.
P = the number of connected components.

The same function as above, represented using the alternative formulation, where each exit point is connected back to the entry point. This graph has 10 edges, 8 nodes, and 1 connected component, which also results in a cyclomatic complexity of 3 using the alternative formulation (10 - 8 + 1 = 3).
An alternative formulation is to use a graph in which each exit point is connected back to the entry point. In this case, the graph is strongly connected, and the cyclomatic complexity of the program is equal to the cyclomatic number of its graph (also known as the first Betti number), which is defined as[2]
M = E - N + P.
This may be seen as calculating the number of linearly independent cycles that exist in the graph, i.e., those cycles that do not contain other cycles within themselves. Note that because each exit point loops back to the entry point, there is at least one such cycle for each exit point.
For a single program (or subroutine or method), P is always equal to 1. So a simpler formula for a single subroutine is
M = E - N + 2
Cyclomatic complexity may, however, be applied to several such programs or subprograms at the same time (e.g., to all of the methods in a class), and in these cases, P will be equal to the number of programs in question, as each subprogram will appear as a disconnected subset of the graph.
McCabe showed that the cyclomatic complexity of any structured program with only one entry point and one exit point is equal to the number of decision points (i.e., "if" statements or conditional loops) contained in that program plus one. However, this is true only for decision points counted at the lowest, machine-level instructions.[4] Decisions involving compound predicates like those found in high-level languages like IF cond1 AND cond2 THEN ... should be counted in terms of predicate variables involved, i.e., in this example, one should count two decision points, because at machine level it is equivalent to IF cond1 THEN IF cond2 THEN [2][5]
Cyclomatic complexity may be extended to a program with multiple exit points; in this case, it is equal to
pi - s + 2,
where pi is the number of decision points in the program, and s is the number of exit points.\cite{complexity}  \textbf{Citato da wikipedia, da modificare tutto}\\

Ghidra offers a class to compute the complexity of a function, CyclomaticCompelxity. This class has a method to calculate the cyclomatic complexity of a function by decomposing it into a flow graph using a BasicBlockModel. During the decompilation, we calculate the complexity of each function and stores it into the .json file. Then we calculate as a feature, the mean, the standard deviation, and the maximum value of complexity for each sample.

\subsection{Standard Library}

One primary task of reverse engineering binary code is to identify library code. Since what the library code does is often known, it is of no interest to an analyst. Hex-Rays has developed the IDA FLIRT signatures to tackle the problem. Function ID is Ghidra's function signature system. Unfortunately, Ghidra has very few Function ID datasets. There is only function identification for the Visual Studio supplied libraries. Ghidra's Function ID allows identifying functions based on hashing the masked function bytes automatically.\cite{ghidra_fid}\\

We exploit this functionality to determine which of the functions belongs to a standard library. Then we calculate the number of standard functions in the given sample and use it as a feature.

\section{Rich Header features}
We used the script in the paper to calculate the rich hash and pv for each of the samples. Sadly, as pointed out in the paper, not every binary is compiled with the rich header; in fact, only 10 samples out of 100 have it. The script extracts the \texttt{Product\_ID}, the \texttt{Product\_Version}, and \texttt{Product\_Count}, we concatenate those numbers with a dash "-" to create a key and set 1 if the sample contains the previous key, 0 otherwise.

